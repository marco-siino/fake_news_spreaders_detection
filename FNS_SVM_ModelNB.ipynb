{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SVM_ModelNB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/fake_news_spreaders_detection/blob/main/SVM_ModelNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hLo5ufkCT1"
      },
      "source": [
        "## Automated Detection of Fake News Spreaders: An Evaluative Study of Transformers and SOTA Models on Multilingual Dataset. \n",
        "Naive Bayes Model, Training and Testing Notebook.\n",
        "Code by M. Siino. \n",
        "\n",
        "From the paper: \"Automated Detection of Fake News Spreaders: An Evaluative Study of Transformers and SOTA Models on Multilingual Dataset.\" by M.Siino et al.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQSunQ-ucjLX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import naive_bayes,svm\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from google.colab import files\n",
        "from io import open\n",
        "from pathlib import Path"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHd_fxmHCfa"
      },
      "source": [
        "## Importing DS and extract in current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocYMUXaY8r0_",
        "outputId": "c4e476cb-ee67-4682-b963-a73f771cc5ce"
      },
      "source": [
        "# Url obtained starting from this: https://drive.google.com/file/d/19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP/ and forcing export=download.\n",
        "urlTrainingSet = \"https://drive.google.com/uc?export=download&id=19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP\"\n",
        "urlTestSet=\"https://drive.google.com/uc?export=download&id=1nLiYvsnqcSPsS27YrBzlFjinmXwidIYa\"\n",
        "\n",
        "training_set = tf.keras.utils.get_file(\"pan20-author-profiling-training-2020-02-23.zip\", urlTrainingSet,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "test_set = tf.keras.utils.get_file(\"pan20-author-profiling-test-2020-02-23.zip\", urlTestSet,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "training_set_dir = os.path.join(os.path.dirname(training_set), 'pan20-author-profiling-training-2020-02-23')\n",
        "test_set_dir = os.path.join(os.path.dirname(test_set), 'pan20-author-profiling-test-2020-02-23')\n",
        "\n",
        "print(training_set)\n",
        "print(training_set_dir)\n",
        "\n",
        "!ls -A"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://drive.google.com/uc?export=download&id=19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP\n",
            "3096576/3094459 [==============================] - 0s 0us/step\n",
            "3104768/3094459 [==============================] - 0s 0us/step\n",
            "Downloading data from https://drive.google.com/uc?export=download&id=1nLiYvsnqcSPsS27YrBzlFjinmXwidIYa\n",
            "2138112/2135236 [==============================] - 0s 0us/step\n",
            "2146304/2135236 [==============================] - 0s 0us/step\n",
            "./pan20-author-profiling-training-2020-02-23.zip\n",
            "./pan20-author-profiling-training-2020-02-23\n",
            ".config\n",
            "__MACOSX\n",
            "pan20-author-profiling-test-2020-02-23\n",
            "pan20-author-profiling-test-2020-02-23.zip\n",
            "pan20-author-profiling-training-2020-02-23\n",
            "pan20-author-profiling-training-2020-02-23.zip\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di7jOZjALo4X"
      },
      "source": [
        "## Build folders hierarchy to use Keras folders preprocessing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ucATWhfGSGf",
        "outputId": "16720b0d-2f5b-438a-9f3b-c7e39f70607b"
      },
      "source": [
        "### Training Folders. ###\n",
        "\n",
        "# First level directory.\n",
        "if not os.path.exists('train_dir_en'):\n",
        "    os.makedirs('train_dir_en')\n",
        "if not os.path.exists('train_dir_es'):\n",
        "    os.makedirs('train_dir_es')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('train_dir_en/0'):\n",
        "    os.makedirs('train_dir_en/0')\n",
        "if not os.path.exists('train_dir_es/0'):\n",
        "    os.makedirs('train_dir_es/0')\n",
        "if not os.path.exists('train_dir_en/1'):\n",
        "    os.makedirs('train_dir_en/1')\n",
        "if not os.path.exists('train_dir_es/1'):\n",
        "    os.makedirs('train_dir_es/1')\n",
        "\n",
        "# Make Py variables.\n",
        "train_dir='train_dir_'\n",
        "\n",
        "## Test Folders. ##\n",
        "# First level directory.\n",
        "if not os.path.exists('test_dir_en'):\n",
        "    os.makedirs('test_dir_en')\n",
        "if not os.path.exists('test_dir_es'):\n",
        "    os.makedirs('test_dir_es')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('test_dir_en/0'):\n",
        "    os.makedirs('test_dir_en/0')\n",
        "if not os.path.exists('test_dir_es/0'):\n",
        "    os.makedirs('test_dir_es/0')\n",
        "if not os.path.exists('test_dir_en/1'):\n",
        "    os.makedirs('test_dir_en/1')\n",
        "if not os.path.exists('test_dir_es/1'):\n",
        "    os.makedirs('test_dir_es/1')\n",
        "\n",
        "# Make Py variables.\n",
        "test_dir='test_dir_'\n",
        "\n",
        "!ls -A"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".config\t\t\t\t\t\tsample_data\n",
            "__MACOSX\t\t\t\t\ttest_dir_en\n",
            "pan20-author-profiling-test-2020-02-23\t\ttest_dir_es\n",
            "pan20-author-profiling-test-2020-02-23.zip\ttrain_dir_en\n",
            "pan20-author-profiling-training-2020-02-23\ttrain_dir_es\n",
            "pan20-author-profiling-training-2020-02-23.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNib56hF_8an"
      },
      "source": [
        "## Set language and directory paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq0EgZuf_5tv"
      },
      "source": [
        "# Set en and es train_dir and test_dir paths.\n",
        "language='es'\n",
        "\n",
        "truth_file_training_dir_es=training_set_dir+'/'+language+'/'\n",
        "truth_file_training_path_es = truth_file_training_dir_es+'truth.txt'\n",
        "\n",
        "truth_file_test_dir=test_set_dir\n",
        "truth_file_test_path_es = truth_file_test_dir+'/'+language+'.txt'\n",
        "\n",
        "\n",
        "language='en'\n",
        "\n",
        "truth_file_training_dir_en=training_set_dir+'/'+language+'/'\n",
        "truth_file_training_path_en = truth_file_training_dir_en+'truth.txt'\n",
        "\n",
        "truth_file_test_path_en = truth_file_test_dir+'/'+language+'.txt'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VQKsc4XOpD8"
      },
      "source": [
        "## Read truth.txt to organize training dataset folders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kxcJ92-Nkto"
      },
      "source": [
        "# Organize EN folders.\n",
        "language='en'\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_training_path_en, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_training_dir_en+fNameXml):\n",
        "      os.rename(truth_file_training_dir_en+fNameXml, './train_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()\n",
        "\n",
        "language='es'\n",
        "# Organize ES folders.\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_training_path_es, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_training_dir_es+fNameXml):\n",
        "      os.rename(truth_file_training_dir_es+fNameXml, './train_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr3aiOLvD_9L"
      },
      "source": [
        "## Read truth.txt to organize test dataset folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoO-K7LyD5_4"
      },
      "source": [
        "#Organize EN folders.\n",
        "language='en'\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_test_path_en, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_test_dir+'/'+language+'/'+fNameXml):\n",
        "      os.rename(truth_file_test_dir+'/'+language+'/'+fNameXml, './test_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()\n",
        "\n",
        "#Organize EN folders.\n",
        "language='es'\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_test_path_es, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_test_dir+'/'+language+'/'+fNameXml):\n",
        "      os.rename(truth_file_test_dir+'/'+language+'/'+fNameXml, './test_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITouXXtQ8WzV"
      },
      "source": [
        "## Function to pre-process source text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDPIqAgXYWim"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, '<\\!\\[CDATA\\[', ' <')\n",
        "  tag_open_document_removed = tf.strings.regex_replace(tag_open_CDATA_removed, '<document>', '')\n",
        "  tag_closed_document_removed = tf.strings.regex_replace(tag_open_document_removed, '</document>\\n(\\t){0,2}', '> ')\n",
        "  tag_open_documents_removed  = tf.strings.regex_replace(tag_closed_document_removed, '<documents>\\n(\\t){0,2}', ' <')\n",
        "  tag_closed_documents_removed = tf.strings.regex_replace(tag_open_documents_removed, '</documents>\\n(\\t){0,2}', '> ')\n",
        "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_closed_documents_removed,'\\]{1,}>', '')\n",
        "  formatting_removed = tf.strings.regex_replace(tag_closed_CDATA_removed, '<author lang=\"es\">\\n\\t', '<author_lang=\"es\">')\n",
        "  formatting_removed = tf.strings.regex_replace(formatting_removed, '<author lang=\"en\">\\n\\t', '<author_lang=\"en\">')\n",
        "  \n",
        "  keeping_tweet_style = tf.strings.regex_replace(formatting_removed, '\\n', ' \\n')\n",
        "  output_data = tf.strings.regex_replace(keeping_tweet_style, 'ht\\w{0,3}:?/{0,2}…', '#URL#')\n",
        "  return output_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQxej8gL9GLQ"
      },
      "source": [
        "## First model's layer: Text Vectorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWh2tZYGYXs4"
      },
      "source": [
        "# Maximum number of words allowed 76000 in our dictionary.\n",
        "max_features = 76000\n",
        "# After tokenization 4060 covers all the document lenghts in our dataset.\n",
        "sequence_length = 4060\n",
        "\n",
        "vectorize_layer_es = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "vectorize_layer_en = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the dataset."
      ],
      "metadata": {
        "id": "MRMgBDgsCua6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=1\n",
        "\n",
        "# Build the dataset for Spanish.\n",
        "language='es'\n",
        "\n",
        "raw_train_ds_es = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir+language, \n",
        "    batch_size=batch_size, \n",
        "    #validation_split=0.0, \n",
        "    #subset='training', \n",
        "    shuffle='false',\n",
        "    seed=1\n",
        "    )\n",
        "\n",
        "train_text = raw_train_ds_es.map(lambda x, y: x)\n",
        "vectorize_layer_es.adapt(train_text)\n",
        "\n",
        "raw_test_ds_es = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    test_dir+language, \n",
        "    batch_size=batch_size,\n",
        "    shuffle='false'\n",
        "    )\n",
        "\n",
        "\n",
        "# Build the dataset for Spanish.\n",
        "language='en'\n",
        "\n",
        "raw_train_ds_en = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir+language, \n",
        "    batch_size=batch_size, \n",
        "    #validation_split=0.0, \n",
        "    #subset='training', \n",
        "    shuffle='false',\n",
        "    seed=1\n",
        "    )\n",
        "\n",
        "train_text = raw_train_ds_en.map(lambda x, y: x)\n",
        "vectorize_layer_en.adapt(train_text)\n",
        "\n",
        "raw_test_ds_en = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    test_dir+language, \n",
        "    batch_size=batch_size,\n",
        "    shuffle='false'\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEu8rdcQCuHy",
        "outputId": "3b695447-0f1c-4340-b5ef-06116a4e6757"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 files belonging to 2 classes.\n",
            "Found 200 files belonging to 2 classes.\n",
            "Found 300 files belonging to 2 classes.\n",
            "Found 200 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition."
      ],
      "metadata": {
        "id": "VgvX5p5QB24X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcbadi5dBF8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d884f4-bdb0-48c7-baa5-e7011f120592"
      },
      "source": [
        "# Word embedding dimensions.\n",
        "embedding_dim = 32\n",
        "\n",
        "model_es = tf.keras.Sequential([\n",
        "                             tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "                             vectorize_layer_es,\n",
        "                             layers.Embedding(max_features + 1, embedding_dim),                     \n",
        "                             layers.Dropout(0.8),\n",
        "\n",
        "                             layers.Conv1D(32,32),\n",
        "                             layers.MaxPooling1D(),\n",
        "                             layers.Dropout(0.5),\n",
        "\n",
        "                             layers.Conv1D(32,16),\n",
        "                             layers.MaxPooling1D(),\n",
        "                             layers.Dropout(0.5),\n",
        "                             \n",
        "                             layers.GlobalAveragePooling1D(),\n",
        "                             layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "model_en = tf.keras.Sequential([\n",
        "                             tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "                             vectorize_layer_en,\n",
        "                             layers.Embedding(max_features + 1, embedding_dim),                     \n",
        "                             layers.Dropout(0.8),\n",
        "\n",
        "                             layers.Conv1D(32,32),\n",
        "                             layers.MaxPooling1D(),\n",
        "                             layers.Dropout(0.5),\n",
        "\n",
        "                             layers.Conv1D(32,16),\n",
        "                             layers.MaxPooling1D(),\n",
        "                             layers.Dropout(0.5),\n",
        "                             \n",
        "                             layers.GlobalAveragePooling1D(),\n",
        "                             layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop()\n",
        "model_es.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer=opt, metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) \n",
        "model_en.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer=opt, metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) \n",
        "model_es.summary()\n",
        "model_en.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 4060)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 4060, 32)          2432032   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4060, 32)          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 4029, 32)          32800     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 2014, 32)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2014, 32)          0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 1999, 32)          16416     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 999, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 999, 32)           0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 32)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,481,281\n",
            "Trainable params: 2,481,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 4060)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 4060, 32)          2432032   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4060, 32)          0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 4029, 32)          32800     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 2014, 32)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 2014, 32)          0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 1999, 32)          16416     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 999, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 999, 32)           0         \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 32)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,481,281\n",
            "Trainable params: 2,481,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA0x6rnqY07p",
        "outputId": "80561621-1321-49e8-c61e-5106675cf7bc"
      },
      "source": [
        "# Train and test with spanish dataset.\n",
        "training_labels=[]\n",
        "training_samples=[]\n",
        "\n",
        "for element in raw_train_ds_es:\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  \n",
        "  #print(\"Sample considered is: \", authorDocument[0])\n",
        "  #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "  #print(\"And has label: \", label[0].numpy())\n",
        "  \n",
        "  text_vect_layer_model = tf.keras.Model(inputs=model_es.input,\n",
        "                                       outputs=model_es.get_layer('text_vectorization').output)\n",
        "  text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "  training_labels.append(label[0].numpy())\n",
        "  current_sample=np.zeros(max_features)\n",
        "  for current_token in text_vect_out[0][:].numpy():\n",
        "    #print(current_token,end=' ')\n",
        "    #print(vectorize_layer.get_vocabulary()[current_token])\n",
        "    current_sample[current_token]+=1\n",
        "  training_samples.append(current_sample)\n",
        "  #break\n",
        "\n",
        "training_labels=np.array(training_labels)\n",
        "training_samples=np.array(training_samples)\n",
        "#print(\"\\nLE LABELS DEI CAMPIONI DI TRAINING SONO:\")\n",
        "#print(training_labels)\n",
        "#print(\"\\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:\")\n",
        "#print(training_samples)\n",
        "\n",
        "# Ora predispongo il test set per vedere l'accuracy finale.\n",
        "\n",
        "test_labels=[]\n",
        "test_samples=[]\n",
        "\n",
        "for element in raw_test_ds_es:\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  \n",
        "  text_vect_layer_model = tf.keras.Model(inputs=model_es.input,\n",
        "                                       outputs=model_es.get_layer('text_vectorization').output)\n",
        "  text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "  test_labels.append(label[0].numpy())\n",
        "  current_sample=np.zeros(max_features)\n",
        "  for current_token in text_vect_out[0][:].numpy():\n",
        "    current_sample[current_token]+=1\n",
        "  test_samples.append(current_sample)\n",
        "\n",
        "test_labels=np.array(test_labels)\n",
        "test_samples=np.array(test_samples)\n",
        "\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(training_samples,training_labels)\n",
        "# predict the labels on training set\n",
        "#predictions_SVM = SVM.predict(training_samples)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "#model_results[key]['SVM_train']=SVM.score(training_samples,training_labels)\n",
        "print(\"SVM Accuracy Score on Spanish Training set -> \",SVM.score(training_samples,training_labels))\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(test_samples)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "#model_results[key][(fold_nr+1)]=SVM.score(test_samples,test_labels)\n",
        "print(\"SVM Accuracy Score on Spanish Test set -> \",SVM.score(test_samples,test_labels))\n",
        "\n",
        "# ****************************************************\n",
        "\n",
        "# Train and test with English dataset.\n",
        "training_labels=[]\n",
        "training_samples=[]\n",
        "\n",
        "for element in raw_train_ds_en:\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  \n",
        "  #print(\"Sample considered is: \", authorDocument[0])\n",
        "  #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "  #print(\"And has label: \", label[0].numpy())\n",
        "  \n",
        "  text_vect_layer_model = tf.keras.Model(inputs=model_en.input,\n",
        "                                       outputs=model_en.get_layer('text_vectorization_1').output)\n",
        "  text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "  training_labels.append(label[0].numpy())\n",
        "  current_sample=np.zeros(max_features)\n",
        "  for current_token in text_vect_out[0][:].numpy():\n",
        "    #print(current_token,end=' ')\n",
        "    #print(vectorize_layer.get_vocabulary()[current_token])\n",
        "    current_sample[current_token]+=1\n",
        "  training_samples.append(current_sample)\n",
        "  #break\n",
        "\n",
        "training_labels=np.array(training_labels)\n",
        "training_samples=np.array(training_samples)\n",
        "#print(\"\\nLE LABELS DEI CAMPIONI DI TRAINING SONO:\")\n",
        "#print(training_labels)\n",
        "#print(\"\\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:\")\n",
        "#print(training_samples)\n",
        "\n",
        "# Ora predispongo il test set per vedere l'accuracy finale.\n",
        "\n",
        "test_labels=[]\n",
        "test_samples=[]\n",
        "\n",
        "for element in raw_test_ds_en:\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  \n",
        "  text_vect_layer_model = tf.keras.Model(inputs=model_en.input,\n",
        "                                       outputs=model_en.get_layer('text_vectorization_1').output)\n",
        "  text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "  test_labels.append(label[0].numpy())\n",
        "  current_sample=np.zeros(max_features)\n",
        "  for current_token in text_vect_out[0][:].numpy():\n",
        "    current_sample[current_token]+=1\n",
        "  test_samples.append(current_sample)\n",
        "\n",
        "test_labels=np.array(test_labels)\n",
        "test_samples=np.array(test_samples)\n",
        "\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(training_samples,training_labels)\n",
        "# predict the labels on training set\n",
        "#predictions_SVM = SVM.predict(training_samples)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "#model_results[key]['SVM_train']=SVM.score(training_samples,training_labels)\n",
        "print(\"\\nSVM Accuracy Score on English Training set -> \",SVM.score(training_samples,training_labels))\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(test_samples)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "#model_results[key][(fold_nr+1)]=SVM.score(test_samples,test_labels)\n",
        "print(\"SVM Accuracy Score on English Test set -> \",SVM.score(test_samples,test_labels))\n",
        "\n",
        "# ****************************************************\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy Score on Spanish Training set ->  1.0\n",
            "SVM Accuracy Score on Spanish Test set ->  0.77\n",
            "\n",
            "SVM Accuracy Score on English Training set ->  0.9966666666666667\n",
            "SVM Accuracy Score on English Test set ->  0.635\n"
          ]
        }
      ]
    }
  ]
}
