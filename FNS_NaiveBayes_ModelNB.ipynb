{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FNS_NaiveBayes_ModelNB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/fake_news_spreaders_detection/blob/main/NaiveBayes_ModelNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hLo5ufkCT1"
      },
      "source": [
        "## Automated Detection of Fake News Spreaders: An Evaluative Study of Transformers and SOTA Models on Multilingual Dataset. \n",
        "Naive Bayes Model, Training and Testing Notebook.\n",
        "Code by M. Siino. \n",
        "\n",
        "From the paper: \"Automated Detection of Fake News Spreaders: An Evaluative Study of Transformers and SOTA Models on Multilingual Dataset.\" by M.Siino et al.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQSunQ-ucjLX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from google.colab import files\n",
        "from io import open\n",
        "from pathlib import Path"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHd_fxmHCfa"
      },
      "source": [
        "## Importing DS and extract in current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocYMUXaY8r0_",
        "outputId": "e61e4b4f-b1f7-4c37-c211-ed977c25e735"
      },
      "source": [
        "# Url obtained starting from this: https://drive.google.com/file/d/19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP/ and forcing export=download.\n",
        "urlTrainingSet = \"https://drive.google.com/uc?export=download&id=19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP\"\n",
        "urlTestSet=\"https://drive.google.com/uc?export=download&id=1nLiYvsnqcSPsS27YrBzlFjinmXwidIYa\"\n",
        "\n",
        "training_set = tf.keras.utils.get_file(\"pan20-author-profiling-training-2020-02-23.zip\", urlTrainingSet,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "test_set = tf.keras.utils.get_file(\"pan20-author-profiling-test-2020-02-23.zip\", urlTestSet,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "training_set_dir = os.path.join(os.path.dirname(training_set), 'pan20-author-profiling-training-2020-02-23')\n",
        "test_set_dir = os.path.join(os.path.dirname(test_set), 'pan20-author-profiling-test-2020-02-23')\n",
        "\n",
        "print(training_set)\n",
        "print(training_set_dir)\n",
        "\n",
        "!ls -A"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://drive.google.com/uc?export=download&id=19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP\n",
            "3096576/3094459 [==============================] - 0s 0us/step\n",
            "3104768/3094459 [==============================] - 0s 0us/step\n",
            "Downloading data from https://drive.google.com/uc?export=download&id=1nLiYvsnqcSPsS27YrBzlFjinmXwidIYa\n",
            "2138112/2135236 [==============================] - 0s 0us/step\n",
            "2146304/2135236 [==============================] - 0s 0us/step\n",
            "./pan20-author-profiling-training-2020-02-23.zip\n",
            "./pan20-author-profiling-training-2020-02-23\n",
            ".config\n",
            "__MACOSX\n",
            "pan20-author-profiling-test-2020-02-23\n",
            "pan20-author-profiling-test-2020-02-23.zip\n",
            "pan20-author-profiling-training-2020-02-23\n",
            "pan20-author-profiling-training-2020-02-23.zip\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di7jOZjALo4X"
      },
      "source": [
        "## Build folders hierarchy to use Keras folders preprocessing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ucATWhfGSGf",
        "outputId": "2a76d268-a109-488c-9087-360e5acdf54c"
      },
      "source": [
        "### Training Folders. ###\n",
        "\n",
        "# First level directory.\n",
        "if not os.path.exists('train_dir_en'):\n",
        "    os.makedirs('train_dir_en')\n",
        "if not os.path.exists('train_dir_es'):\n",
        "    os.makedirs('train_dir_es')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('train_dir_en/0'):\n",
        "    os.makedirs('train_dir_en/0')\n",
        "if not os.path.exists('train_dir_es/0'):\n",
        "    os.makedirs('train_dir_es/0')\n",
        "if not os.path.exists('train_dir_en/1'):\n",
        "    os.makedirs('train_dir_en/1')\n",
        "if not os.path.exists('train_dir_es/1'):\n",
        "    os.makedirs('train_dir_es/1')\n",
        "\n",
        "# Make Py variables.\n",
        "train_dir='train_dir_'\n",
        "\n",
        "## Test Folders. ##\n",
        "# First level directory.\n",
        "if not os.path.exists('test_dir_en'):\n",
        "    os.makedirs('test_dir_en')\n",
        "if not os.path.exists('test_dir_es'):\n",
        "    os.makedirs('test_dir_es')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('test_dir_en/0'):\n",
        "    os.makedirs('test_dir_en/0')\n",
        "if not os.path.exists('test_dir_es/0'):\n",
        "    os.makedirs('test_dir_es/0')\n",
        "if not os.path.exists('test_dir_en/1'):\n",
        "    os.makedirs('test_dir_en/1')\n",
        "if not os.path.exists('test_dir_es/1'):\n",
        "    os.makedirs('test_dir_es/1')\n",
        "\n",
        "# Make Py variables.\n",
        "test_dir='test_dir_'\n",
        "\n",
        "!ls -A"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".config\t\t\t\t\t\tsample_data\n",
            "__MACOSX\t\t\t\t\ttest_dir_en\n",
            "pan20-author-profiling-test-2020-02-23\t\ttest_dir_es\n",
            "pan20-author-profiling-test-2020-02-23.zip\ttrain_dir_en\n",
            "pan20-author-profiling-training-2020-02-23\ttrain_dir_es\n",
            "pan20-author-profiling-training-2020-02-23.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNib56hF_8an"
      },
      "source": [
        "## Set language and directory paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq0EgZuf_5tv"
      },
      "source": [
        "# Set en and es train_dir and test_dir paths.\n",
        "language='es'\n",
        "\n",
        "truth_file_training_dir_es=training_set_dir+'/'+language+'/'\n",
        "truth_file_training_path_es = truth_file_training_dir_es+'truth.txt'\n",
        "\n",
        "truth_file_test_dir=test_set_dir\n",
        "truth_file_test_path_es = truth_file_test_dir+'/'+language+'.txt'\n",
        "\n",
        "\n",
        "language='en'\n",
        "\n",
        "truth_file_training_dir_en=training_set_dir+'/'+language+'/'\n",
        "truth_file_training_path_en = truth_file_training_dir_en+'truth.txt'\n",
        "\n",
        "truth_file_test_path_en = truth_file_test_dir+'/'+language+'.txt'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VQKsc4XOpD8"
      },
      "source": [
        "## Read truth.txt to organize training dataset folders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kxcJ92-Nkto"
      },
      "source": [
        "# Organize EN folders.\n",
        "language='en'\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_training_path_en, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_training_dir_en+fNameXml):\n",
        "      os.rename(truth_file_training_dir_en+fNameXml, './train_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()\n",
        "\n",
        "language='es'\n",
        "# Organize ES folders.\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_training_path_es, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_training_dir_es+fNameXml):\n",
        "      os.rename(truth_file_training_dir_es+fNameXml, './train_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr3aiOLvD_9L"
      },
      "source": [
        "## Read truth.txt to organize test dataset folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoO-K7LyD5_4"
      },
      "source": [
        "#Organize EN folders.\n",
        "language='en'\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_test_path_en, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_test_dir+'/'+language+'/'+fNameXml):\n",
        "      os.rename(truth_file_test_dir+'/'+language+'/'+fNameXml, './test_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()\n",
        "\n",
        "#Organize EN folders.\n",
        "language='es'\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_test_path_es, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_test_dir+'/'+language+'/'+fNameXml):\n",
        "      os.rename(truth_file_test_dir+'/'+language+'/'+fNameXml, './test_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITouXXtQ8WzV"
      },
      "source": [
        "## Function to pre-process source text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDPIqAgXYWim"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, '<\\!\\[CDATA\\[', ' ')\n",
        "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_open_CDATA_removed,'\\]{1,}>', ' ')\n",
        "  tag_author_lang_es_removed = tf.strings.regex_replace(tag_closed_CDATA_removed,'<author lang=\"es\">', ' ')\n",
        "  tag_author_lang_en_removed = tf.strings.regex_replace(tag_author_lang_es_removed,'<author lang=\"en\">', ' ')\n",
        "  tag_closed_author_removed = tf.strings.regex_replace(tag_author_lang_en_removed,'</author>', ' ')\n",
        "  tag_open_documents_removed = tf.strings.regex_replace(tag_closed_author_removed,'<documents>\\n(\\t){0,2}', '')\n",
        "  output_data = tf.strings.regex_replace(tag_open_documents_removed,'</documents>\\n(\\t){0,2}', ' ')\n",
        "  return output_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQxej8gL9GLQ"
      },
      "source": [
        "## First model's layer: Text Vectorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWh2tZYGYXs4"
      },
      "source": [
        "# Maximum number of words allowed 76000 in our dictionary.\n",
        "max_features = 76000\n",
        "# After tokenization 4060 covers all the document lenghts in our dataset.\n",
        "sequence_length = 4060\n",
        "\n",
        "vectorize_layer_es = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "vectorize_layer_en = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the dataset."
      ],
      "metadata": {
        "id": "MRMgBDgsCua6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=1\n",
        "\n",
        "# Build the dataset for Spanish.\n",
        "language='es'\n",
        "\n",
        "raw_train_ds_es = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir+language, \n",
        "    batch_size=batch_size, \n",
        "    #validation_split=0.0, \n",
        "    #subset='training', \n",
        "    shuffle='false',\n",
        "    seed=1\n",
        "    )\n",
        "\n",
        "train_text = raw_train_ds_es.map(lambda x, y: x)\n",
        "vectorize_layer_es.adapt(train_text)\n",
        "\n",
        "raw_test_ds_es = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    test_dir+language, \n",
        "    batch_size=batch_size,\n",
        "    shuffle='false'\n",
        "    )\n",
        "\n",
        "\n",
        "# Build the dataset for Spanish.\n",
        "language='en'\n",
        "\n",
        "raw_train_ds_en = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir+language, \n",
        "    batch_size=batch_size, \n",
        "    #validation_split=0.0, \n",
        "    #subset='training', \n",
        "    shuffle='false',\n",
        "    seed=1\n",
        "    )\n",
        "\n",
        "train_text = raw_train_ds_en.map(lambda x, y: x)\n",
        "vectorize_layer_en.adapt(train_text)\n",
        "\n",
        "raw_test_ds_en = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    test_dir+language, \n",
        "    batch_size=batch_size,\n",
        "    shuffle='false'\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEu8rdcQCuHy",
        "outputId": "ab1aed03-80ce-4f3e-8432-feb2bc074573"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 files belonging to 2 classes.\n",
            "Found 200 files belonging to 2 classes.\n",
            "Found 300 files belonging to 2 classes.\n",
            "Found 200 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition."
      ],
      "metadata": {
        "id": "VgvX5p5QB24X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcbadi5dBF8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed67753-3f8f-41cb-c8dd-2bbd3183644c"
      },
      "source": [
        "# Word embedding dimensions.\n",
        "embedding_dim = 32\n",
        "\n",
        "model_es = tf.keras.Sequential([\n",
        "                             tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "                             vectorize_layer_es,\n",
        "                             layers.Embedding(max_features + 1, embedding_dim),                     \n",
        "                             layers.Dropout(0.8),\n",
        "\n",
        "                             layers.Conv1D(32,32),\n",
        "                             layers.MaxPooling1D(),\n",
        "                             layers.Dropout(0.5),\n",
        "\n",
        "                             layers.Conv1D(32,16),\n",
        "                             layers.MaxPooling1D(),\n",
        "                             layers.Dropout(0.5),\n",
        "                             \n",
        "                             layers.GlobalAveragePooling1D(),\n",
        "                             layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "model_en = tf.keras.Sequential([\n",
        "                             tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "                             vectorize_layer_en,\n",
        "                             layers.Embedding(max_features + 1, embedding_dim),                     \n",
        "                             layers.Dropout(0.8),\n",
        "\n",
        "                             layers.Conv1D(32,32),\n",
        "                             layers.MaxPooling1D(),\n",
        "                             layers.Dropout(0.5),\n",
        "\n",
        "                             layers.Conv1D(32,16),\n",
        "                             layers.MaxPooling1D(),\n",
        "                             layers.Dropout(0.5),\n",
        "                             \n",
        "                             layers.GlobalAveragePooling1D(),\n",
        "                             layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop()\n",
        "model_es.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer=opt, metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) \n",
        "model_en.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer=opt, metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) \n",
        "model_es.summary()\n",
        "model_en.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 4060)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 4060, 32)          2432032   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4060, 32)          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 4029, 32)          32800     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 2014, 32)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2014, 32)          0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 1999, 32)          16416     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 999, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 999, 32)           0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 32)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,481,281\n",
            "Trainable params: 2,481,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 4060)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 4060, 32)          2432032   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4060, 32)          0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 4029, 32)          32800     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 2014, 32)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 2014, 32)          0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 1999, 32)          16416     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 999, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 999, 32)           0         \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 32)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,481,281\n",
            "Trainable params: 2,481,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA0x6rnqY07p",
        "outputId": "287f3fd7-30f5-4736-dbd9-bc85a3f4186d"
      },
      "source": [
        "# Train and test with spanish dataset.\n",
        "training_labels=[]\n",
        "training_samples=[]\n",
        "\n",
        "for element in raw_train_ds_es:\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  \n",
        "  #print(\"Sample considered is: \", authorDocument[0])\n",
        "  #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "  #print(\"And has label: \", label[0].numpy())\n",
        "  \n",
        "  text_vect_layer_model = tf.keras.Model(inputs=model_es.input,\n",
        "                                       outputs=model_es.get_layer('text_vectorization').output)\n",
        "  text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "  training_labels.append(label[0].numpy())\n",
        "  current_sample=np.zeros(max_features)\n",
        "  for current_token in text_vect_out[0][:].numpy():\n",
        "    #print(current_token,end=' ')\n",
        "    #print(vectorize_layer.get_vocabulary()[current_token])\n",
        "    current_sample[current_token]+=1\n",
        "  training_samples.append(current_sample)\n",
        "  #break\n",
        "\n",
        "training_labels=np.array(training_labels)\n",
        "training_samples=np.array(training_samples)\n",
        "#print(\"\\nLE LABELS DEI CAMPIONI DI TRAINING SONO:\")\n",
        "#print(training_labels)\n",
        "#print(\"\\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:\")\n",
        "#print(training_samples)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(training_samples, training_labels)\n",
        "print(\"\\nAccuracy on Spanish training set is: \",clf.score(training_samples,training_labels))\n",
        "\n",
        "# Ora predispongo il test set per vedere l'accuracy finale.\n",
        "\n",
        "test_labels=[]\n",
        "test_samples=[]\n",
        "\n",
        "for element in raw_test_ds_es:\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  \n",
        "  text_vect_layer_model = tf.keras.Model(inputs=model_es.input,\n",
        "                                       outputs=model_es.get_layer('text_vectorization').output)\n",
        "  text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "  test_labels.append(label[0].numpy())\n",
        "  current_sample=np.zeros(max_features)\n",
        "  for current_token in text_vect_out[0][:].numpy():\n",
        "    current_sample[current_token]+=1\n",
        "  test_samples.append(current_sample)\n",
        "\n",
        "test_labels=np.array(test_labels)\n",
        "test_samples=np.array(test_samples)\n",
        "\n",
        "print(\"Accuracy on Spanish test set is: \",clf.score(test_samples,test_labels))\n",
        "\n",
        "# ****************************************************\n",
        "\n",
        "# Now train and test with English dataset.\n",
        "training_labels=[]\n",
        "training_samples=[]\n",
        "\n",
        "for element in raw_train_ds_en:\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  \n",
        "  #print(\"Sample considered is: \", authorDocument[0])\n",
        "  #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "  #print(\"And has label: \", label[0].numpy())\n",
        "  \n",
        "  text_vect_layer_model = tf.keras.Model(inputs=model_en.input,\n",
        "                                       outputs=model_en.get_layer('text_vectorization_1').output)\n",
        "  text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "  training_labels.append(label[0].numpy())\n",
        "  current_sample=np.zeros(max_features)\n",
        "  for current_token in text_vect_out[0][:].numpy():\n",
        "    #print(current_token,end=' ')\n",
        "    #print(vectorize_layer.get_vocabulary()[current_token])\n",
        "    current_sample[current_token]+=1\n",
        "  training_samples.append(current_sample)\n",
        "  #break\n",
        "\n",
        "training_labels=np.array(training_labels)\n",
        "training_samples=np.array(training_samples)\n",
        "#print(\"\\nLE LABELS DEI CAMPIONI DI TRAINING SONO:\")\n",
        "#print(training_labels)\n",
        "#print(\"\\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:\")\n",
        "#print(training_samples)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(training_samples, training_labels)\n",
        "print(\"\\nAccuracy on English training set is: \",clf.score(training_samples,training_labels))\n",
        "\n",
        "# Ora predispongo il test set per vedere l'accuracy finale.\n",
        "\n",
        "test_labels=[]\n",
        "test_samples=[]\n",
        "\n",
        "for element in raw_test_ds_en:\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  \n",
        "  text_vect_layer_model = tf.keras.Model(inputs=model_en.input,\n",
        "                                       outputs=model_en.get_layer('text_vectorization_1').output)\n",
        "  text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "  test_labels.append(label[0].numpy())\n",
        "  current_sample=np.zeros(max_features)\n",
        "  for current_token in text_vect_out[0][:].numpy():\n",
        "    current_sample[current_token]+=1\n",
        "  test_samples.append(current_sample)\n",
        "\n",
        "test_labels=np.array(test_labels)\n",
        "test_samples=np.array(test_samples)\n",
        "\n",
        "print(\"Accuracy on English test set is: \",clf.score(test_samples,test_labels))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy on Spanish training set is:  0.94\n",
            "Accuracy on Spanish test set is:  0.695\n",
            "\n",
            "Accuracy on English training set is:  0.9833333333333333\n",
            "Accuracy on English test set is:  0.695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistic (no need to execute)."
      ],
      "metadata": {
        "id": "-5UCfyHxFyJ7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOlJJMePs7rT"
      },
      "source": [
        "# Distribuzione di probabilità dei samples nel test set.\n",
        "clf.predict_log_proba(test_samples)\n",
        "\n",
        "# Distribuzione di probabilità di un singolo sample.\n",
        "clf.predict_log_proba(text_vect_out[0][:].numpy().reshape(1,-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN2o34QQVTW8",
        "outputId": "e120e9e8-7b00-4eb6-9f82-d58b415067df"
      },
      "source": [
        "ten_most_frequent_ngram_label0=clf.feature_count_[0]\n",
        "ten_most_frequent_ngram_label1=clf.feature_count_[1]\n",
        "\n",
        "maximum0=heapq.nlargest(400, range(len(ten_most_frequent_ngram_label0)), ten_most_frequent_ngram_label0.take)\n",
        "maximum1=heapq.nlargest(400, range(len(ten_most_frequent_ngram_label1)), ten_most_frequent_ngram_label1.take)\n",
        "import heapq\n",
        "\n",
        "print(maximum0)\n",
        "print(maximum1)\n",
        "\n",
        "max0_not_in_max1=set(maximum0)-set(maximum1)\n",
        "max1_not_in_max0=set(maximum1)-set(maximum0)\n",
        "wordNr=1\n",
        "print(\"\\nLe 50 parole più probabili per la label 0 sono:\")\n",
        "for word in max0_not_in_max1:\n",
        "  print(wordNr,\" \",vectorize_layer.get_vocabulary()[word])\n",
        "  wordNr+=1\n",
        "\n",
        "\n",
        "wordNr=1\n",
        "print(\"\\nLe 50 parole più probabili per la label 1 sono:\")\n",
        "for word in max1_not_in_max0:\n",
        "  print(wordNr,\" \",vectorize_layer.get_vocabulary()[word])\n",
        "  wordNr+=1\n",
        "\n",
        "print(clf.feature_log_prob_[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 3, 4, 5, 6, 7, 9, 8, 10, 11, 14, 16, 13, 15, 12, 17, 21, 18, 19, 20, 22, 23, 25, 24, 27, 28, 26, 33, 30, 32, 46, 38, 34, 35, 41, 29, 40, 45, 48, 50, 43, 66, 31, 54, 39, 52, 42, 44, 61, 62, 56, 57, 60, 51, 59, 47, 55, 64, 69, 49, 63, 70, 82, 53, 84, 58, 74, 79, 71, 115, 67, 80, 77, 72, 92, 97, 76, 128, 95, 88, 86, 90, 102, 89, 87, 85, 107, 91, 75, 109, 110, 101, 78, 99, 73, 106, 117, 123, 166, 127, 100, 98, 116, 118, 132, 131, 129, 93, 126, 143, 146, 96, 122, 148, 121, 139, 81, 135, 140, 111, 112, 113, 114, 124, 147, 104, 141, 168, 182, 103, 177, 144, 94, 145, 154, 162, 120, 137, 160, 169, 83, 108, 155, 158, 130, 151, 199, 206, 119, 138, 150, 152, 149, 209, 246, 181, 211, 156, 171, 315, 221, 201, 192, 217, 133, 208, 65, 157, 179, 216, 185, 212, 105, 187, 231, 341, 163, 183, 210, 237, 348, 142, 174, 175, 196, 178, 203, 215, 230, 240, 262, 189, 161, 173, 205, 167, 190, 200, 219, 265, 170, 229, 180, 193, 197, 214, 198, 220, 225, 228, 232, 267, 134, 153, 223, 244, 273, 279, 323, 186, 227, 165, 276, 159, 293, 313, 188, 194, 242, 254, 164, 207, 311, 136, 195, 235, 252, 281, 289, 334, 268, 278, 285, 322, 421, 243, 259, 305, 176, 238, 282, 338, 269, 309, 328, 350, 359, 274, 292, 255, 263, 266, 294, 308, 351, 184, 204, 290, 302, 331, 343, 380, 414, 234, 301, 332, 389, 125, 264, 297, 320, 373, 455, 226, 239, 352, 362, 384, 236, 245, 270, 287, 314, 318, 337, 340, 342, 370, 582, 253, 260, 283, 316, 360, 404, 551, 191, 251, 271, 317, 365, 366, 381, 436, 580, 233, 256, 280, 298, 383, 398, 417, 424, 481, 299, 307, 319, 353, 432, 645, 247, 272, 286, 288, 306, 339, 488, 249, 261, 371, 386, 456, 459, 484, 275, 344, 388, 449, 695, 241, 325, 327, 335, 349, 379, 387, 399, 402, 451, 628, 690, 222, 312, 368, 390, 422, 431, 476, 479, 499, 501, 531, 676, 677, 693, 372, 393, 420, 427, 536, 296, 415, 416, 448, 466, 541, 347, 364, 374, 378, 400, 409, 423, 426, 438, 470, 489, 515, 617]\n",
            "[0, 2, 3, 4, 5, 7, 6, 8, 9, 12, 13, 15, 17, 19, 10, 11, 18, 20, 14, 31, 24, 23, 36, 21, 29, 37, 22, 25, 16, 32, 30, 26, 34, 28, 35, 33, 39, 68, 38, 40, 65, 42, 49, 53, 47, 27, 44, 51, 58, 43, 55, 41, 45, 73, 52, 83, 48, 50, 59, 63, 57, 67, 81, 60, 56, 75, 72, 54, 78, 71, 94, 64, 105, 76, 70, 93, 96, 125, 62, 74, 61, 103, 77, 108, 85, 104, 69, 86, 87, 98, 80, 119, 89, 172, 100, 136, 90, 88, 79, 91, 134, 120, 111, 112, 113, 114, 99, 218, 121, 202, 133, 122, 101, 106, 95, 124, 142, 116, 130, 153, 118, 224, 92, 258, 159, 164, 138, 250, 248, 82, 291, 84, 126, 165, 102, 213, 300, 303, 304, 191, 117, 176, 277, 107, 184, 137, 330, 97, 161, 123, 167, 129, 149, 170, 163, 222, 109, 150, 152, 157, 135, 156, 188, 204, 110, 173, 195, 194, 131, 141, 186, 139, 151, 397, 140, 144, 145, 174, 180, 395, 127, 132, 257, 175, 226, 233, 241, 329, 408, 418, 155, 178, 207, 406, 198, 284, 446, 154, 171, 190, 193, 369, 197, 247, 249, 158, 189, 236, 295, 333, 183, 234, 324, 358, 405, 147, 239, 179, 200, 245, 326, 185, 187, 251, 261, 143, 160, 256, 505, 508, 196, 253, 465, 518, 162, 205, 272, 275, 296, 548, 146, 148, 235, 238, 260, 310, 336, 169, 203, 321, 564, 581, 181, 192, 214, 271, 419, 433, 220, 223, 243, 255, 270, 286, 288, 345, 605, 227, 264, 280, 363, 563, 46, 225, 312, 357, 66, 283, 354, 394, 435, 631, 215, 219, 228, 242, 263, 266, 299, 619, 168, 210, 287, 298, 306, 361, 376, 670, 201, 232, 252, 269, 307, 325, 327, 346, 412, 517, 547, 657, 259, 356, 411, 672, 208, 254, 274, 347, 382, 434, 212, 229, 268, 290, 319, 335, 355, 428, 297, 317, 392, 463, 576, 642, 761, 216, 244, 294, 314, 316, 367, 282, 292, 301, 318, 401, 430, 598, 634, 783, 177, 230, 302, 344, 364, 407, 437, 464, 757, 807, 822, 823, 209, 278, 339, 349, 375, 377, 425, 457, 668, 694, 849, 199, 211, 217, 281, 285, 320, 374, 403, 413, 445, 486, 490, 513, 524, 557, 620, 871, 874, 289, 308, 372, 378, 385, 391, 528, 533, 545]\n",
            "\n",
            "Le 50 parole più probabili per la label 0 sono:\n",
            "1   has\n",
            "2   crear\n",
            "3   #HASHTAG#:\n",
            "4   haya\n",
            "5   sentido\n",
            "6   Android\n",
            "7   ⤵\n",
            "8   creo\n",
            "9   #HASHTAG#.\n",
            "10   artículo\n",
            "11   #HASHTAG#,\n",
            "12   <✏\n",
            "13   ➡️\n",
            "14   …>\n",
            "15   RT\n",
            "16   Galaxy\n",
            "17   Albacete\n",
            "18   he\n",
            "19   <RECOMENDACIONES\n",
            "20   #USER#…>\n",
            "21   algo\n",
            "22   cuenta\n",
            "23   Hoy\n",
            "24   aquí\n",
            "25   #USER#,\n",
            "26   Pero\n",
            "27   decir\n",
            "28   cualquier\n",
            "29   hecho\n",
            "30   estado\n",
            "31   nuestros\n",
            "32   alguien\n",
            "33   información\n",
            "34   pasado\n",
            "35   haber\n",
            "36   Ya\n",
            "37   #HASHTAG#-T>\n",
            "38   mañana\n",
            "39   hemos\n",
            "40   ir\n",
            "41   tanto\n",
            "42   debe\n",
            "43   esa\n",
            "44   tenemos\n",
            "45   falta\n",
            "46   campaña\n",
            "47   (#HASHTAG#)>\n",
            "48   veces\n",
            "49   través\n",
            "50   <(#HASHTAG#)\n",
            "51   d\n",
            "52   muchas\n",
            "53   momento\n",
            "54   lugar\n",
            "55   sí\n",
            "56   pasa\n",
            "57   acuerdo\n",
            "58   Hay\n",
            "59   Francisco\n",
            "60   verdad\n",
            "61   política\n",
            "62   hora\n",
            "63   tipo\n",
            "64   dicen\n",
            "65   datos\n",
            "66   Mi\n",
            "67   sino\n",
            "68   seguridad\n",
            "69   frente\n",
            "70   cambio\n",
            "71   <Y\n",
            "72   ¿Qué\n",
            "73   seguir\n",
            "74   final\n",
            "75   sociales\n",
            "76   sistema\n",
            "77   nadie\n",
            "78   ellos\n",
            "79   buena\n",
            "80   dijo\n",
            "81   servicio\n",
            "82   muchos\n",
            "83   buen\n",
            "84   Papa\n",
            "85   Navidad\n",
            "86   <Buenos\n",
            "87   sabe\n",
            "88   paso\n",
            "89   parece\n",
            "90   fuera\n",
            "91   empresas\n",
            "92   <Hoy\n",
            "93   \"El\n",
            "94   Google\n",
            "95   via\n",
            "96   unas\n",
            "97   realidad\n",
            "98   políticos\n",
            "99   #USER#.\n",
            "100   tú\n",
            "101   vamos\n",
            "102   quién\n",
            "103   estoy\n",
            "104   tengo\n",
            "105   Aquí\n",
            "106   móvil\n",
            "107   equipo\n",
            "108   Pues\n",
            "109   Málaga\n",
            "110   red\n",
            "111   de…>\n",
            "\n",
            "Le 50 parole più probabili per la label 1 sono:\n",
            "1   noticias\n",
            "2   gustado\n",
            "3   Lapiz\n",
            "4   policía\n",
            "5   feliz\n",
            "6   Nuevo\n",
            "7   Venezuela\n",
            "8   <Muere\n",
            "9   –\n",
            "10   Unete\n",
            "11   <DESCARGAR\n",
            "12   jóvenes\n",
            "13   Dominicana\n",
            "14   Conciente\n",
            "15   Valencia\n",
            "16   <VIDEO\n",
            "17   (2015)\n",
            "18   mata\n",
            "19   OLVIDES\n",
            "20   Don\n",
            "21   vivo\n",
            "22   (Official\n",
            "23   secreto\n",
            "24   Mundo\n",
            "25   Mozart\n",
            "26   cerebro\n",
            "27   Video)\n",
            "28   Joven\n",
            "29   (#URL#\n",
            "30   @\n",
            "31   lista\n",
            "32   <Se\n",
            "33   <#URL#\n",
            "34   Su\n",
            "35   Noticias\n",
            "36   añadido\n",
            "37   Accidente\n",
            "38   reproducción\n",
            "39   <Me\n",
            "40   Mayor\n",
            "41   <Remedios\n",
            "42   &amp;\n",
            "43   6\n",
            "44   <DE\n",
            "45   EL\n",
            "46   ¡COMPÁRTELO!\n",
            "47   ULTIMO\n",
            "48   MINUTO\n",
            "49   Clasico\n",
            "50   hijo\n",
            "51   Ft\n",
            "52   Años\n",
            "53   muerte\n",
            "54   cáncer\n",
            "55   Barcelona\n",
            "56   Le\n",
            "57   <ESTRENO\n",
            "58   Dios\n",
            "59   2019\n",
            "60   Miguelo\n",
            "61   amor\n",
            "62   Podemos\n",
            "63   amigos\n",
            "64   Estado\n",
            "65   9\n",
            "66   15\n",
            "67   video\n",
            "68   Imagenes)\n",
            "69   meses\n",
            "70   (Fuertes\n",
            "71   Policía\n",
            "72   30\n",
            "73   publicar\n",
            "74   calle\n",
            "75   Nacional\n",
            "76   Civil\n",
            "77   Europa\n",
            "78   programa\n",
            "79   pueblo\n",
            "80   hombres\n",
            "81   cárcel\n",
            "82   Vida\n",
            "83   (Video\n",
            "84   habla\n",
            "85   ciudad\n",
            "86   Tu\n",
            "87   Del\n",
            "88   menores\n",
            "89   Follow\n",
            "90   cuerpo\n",
            "91   <Que\n",
            "92   <Este\n",
            "93   Oficial)\n",
            "94   Nueva\n",
            "95   nuevas\n",
            "96   de...\n",
            "97   <Lo\n",
            "98   “El\n",
            "99   Iglesias\n",
            "100   Hola\n",
            "101   2020\n",
            "102   Carlos\n",
            "103   Alfa\n",
            "104   salud\n",
            "105   EN\n",
            "106   <Esta\n",
            "107   <Acaba\n",
            "108   cinco\n",
            "109   niña\n",
            "110   <[\n",
            "111   #HASHTAG#]\n",
            "[ -0.61713669 -13.26278597  -4.12954265 ... -13.26278597 -13.26278597\n",
            " -13.26278597]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
